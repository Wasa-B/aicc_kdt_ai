import os
import faiss
import numpy as np
from PyPDF2 import PdfReader
from openai import OpenAI
import pickle
import hashlib


from dotenv import load_dotenv
load_dotenv()
API_KEY = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=API_KEY)

# -------------------------
# 1. PDF ì½ê¸°
# -------------------------
def read_pdf(file_path):
    reader = PdfReader(file_path)
    texts = []
    for page in reader.pages:
        text = page.extract_text()
        if text:
            texts.append(text)
    return "\n".join(texts)

# -------------------------
# 2. í…ìŠ¤íŠ¸ Chunking
# -------------------------
def chunk_text(text, max_tokens=200):
    """
    ê¸´ í…ìŠ¤íŠ¸ë¥¼ max_tokens ë‹¨ìœ„ë¡œ ë‚˜ëˆ”.
    """
    import tiktoken
    enc = tiktoken.get_encoding("cl100k_base")
    words = text.split()
    chunks = []
    current_chunk = []

    current_tokens = 0
    for word in words:
        tokens = len(enc.encode(word + " "))
        if current_tokens + tokens > max_tokens:
            chunks.append(" ".join(current_chunk))
            current_chunk = [word]
            current_tokens = tokens
        else:
            current_chunk.append(word)
            current_tokens += tokens

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

# -------------------------
# 3. Embedding ìƒì„±
# -------------------------
def embed_texts(texts):
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=texts
    )
    embeddings = [d.embedding for d in response.data]
    return embeddings

# -------------------------
# 4. ê²€ìƒ‰
# -------------------------
class VectorStore:
    def __init__(self, embeddings, texts):
        self.texts = texts
        dim = len(embeddings[0])
        self.index = faiss.IndexFlatL2(dim)
        self.index.add(np.array(embeddings).astype('float32'))

    def search(self, query_embedding, k=3):
        D, I = self.index.search(np.array([query_embedding]).astype('float32'), k)
        results = [self.texts[i] for i in I[0]]
        return results

# -------------------------
# 5. GPT ë‹µë³€ ìƒì„±
# -------------------------
def generate_answer(question, retrieved_chunks, prompt: str = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ PDF ë¬¸ì„œ ì§ˆë¬¸ ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.", model = "gpt-3.5-turbo"):
    context = "\n\n".join(retrieved_chunks)
    prompt = f"""
ë‹¤ìŒ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ì •ë³´:
{context}

ì§ˆë¬¸:
{question}
"""
    completion = client.chat.completions.create(
        model=model,
        # model="gpt-4o",
        messages=[
            {"role": "system", "content": prompt},
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=1000,
    )
    
    return completion.choices[0].message.content.strip()

# -------------------------
# PDF í•´ì‹œ ìƒì„±
# -------------------------
def file_hash(file_path):
    h = hashlib.sha256()
    with open(file_path, "rb") as f:
        h.update(f.read())
    return h.hexdigest()

# -------------------------
# ë©”ì¸ ì‹¤í–‰
# -------------------------

def rag_question(pdf_path: str, question: str,prompt:str = "ë‹¹ì‹ ì€ ì¹œì ˆí•œ PDF ë¬¸ì„œ ì§ˆë¬¸ ë‹µë³€ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.",model = "gpt-3.5-turbo", max_tokens: int = 200, k: int = 3):
    # 1. PDF ì½ê¸°
    hash_id = file_hash(pdf_path)
    cache_file = f"./datas/cache_{hash_id}.pkl"
    # 2. Chunking
    if os.path.exists(cache_file):
        print(f"ğŸ“‚ ìºì‹œ ë¡œë“œ: {cache_file}")
        with open(cache_file, "rb") as f:
            data = pickle.load(f)
        chunks = data["chunks"]
        chunk_embeddings = data["embeddings"]
    else:
        print("ğŸš€ PDF ì²˜ë¦¬ ì¤‘...")
        raw_text = read_pdf(pdf_path)
        chunks = chunk_text(raw_text, max_tokens)
        chunk_embeddings = embed_texts(chunks)
        with open(cache_file, "wb") as f:
            pickle.dump({"chunks": chunks, "embeddings": chunk_embeddings}, f)
        print(f"âœ… ìºì‹œ ì €ì¥ ì™„ë£Œ: {cache_file}")
    
    # 4. Vector Store ì´ˆê¸°í™”
    store = VectorStore(chunk_embeddings, chunks)
    # 5. ì§ˆë¬¸ ì²˜ë¦¬
    query_embedding = embed_texts([question])[0]
    relevant_chunks = store.search(query_embedding, k)
    answer = generate_answer(question, relevant_chunks,prompt,model)
    return answer

if __name__ == "__main__":
    pdf_path = "./doc/ê²½ì£¼ ê´€ê´‘ ì¸í”Œë£¨ì–¸ì„œ ë§ˆì¼€íŒ… 3ê°œë…„ ì „ëµ ë¡œë“œë§µ.pdf"
    answer = rag_question(pdf_path, 
                            "ë¡œì»¬ ì¸í”Œë£¨ì–¸ì„œ ë¯¸ë‚˜ì˜ 1ì›” ê³„íšì„ ì°¸ê³ í•˜ì—¬ ì„¸ë¶€ ê³„íšì„ ì„¤ê³„í•´ì¤˜"
                          "ë‹¹ì‹ ì€ pdfíŒŒì¼ ë¶„ì„ ì „ë¬¸ê°€ ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ pdfì—ì„œ ì°¾ì•„ ì˜ ì •ë¦¬ í•˜ì—¬ ì•Œë ¤ì£¼ëŠ” ê²ƒì— íŠ¹í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
    print(answer)